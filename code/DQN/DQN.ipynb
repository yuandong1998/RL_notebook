{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN\n",
    "-------------\n",
    "\n",
    "这个项目是pytorch文档里的[REINFORCEMENT LEARNING (DQN) TUTORIAL](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)。\n",
    "\n",
    "\n",
    "算法介绍：\n",
    "\n",
    "首先如果采用online Q-Learning的方式，通过$\\epsilon$-greedy策略采样a，然后观察$(s,a,r,s')$。再通过$\\Delta \\mathbf{w}=\\alpha\\left(r+\\gamma \\max _{a^{\\prime}} \\hat{q}\\left(s^{\\prime}, a^{\\prime}, \\mathbf{w}\\right)-\\hat{q}(s, a, \\mathbf{w})\\right) \\nabla_{\\mathbf{w}} \\hat{q}(s, a, \\mathbf{w})$更新参数，这样会导致序列state有很强的相关性，并且target value 会一直变化。\n",
    "\n",
    "​\t\t所以提出了DQN算法，采用了**experience replay** 和 **target network** 来解决上述问题，DQN的算法流程如下所示：\n",
    "\n",
    "<img src=\"https://cdn.mathpix.com/snip/images/DJAT5cf-ZOB65SXSwyrCi0RDSiiaVtw7B_j6deRlqsU.original.fullsize.png\" style=\"zoom: 67%;\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped # unwrapped是打开限制的意思，据说不做这个动作会有很多限制\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory\n",
    "-------------\n",
    "\n",
    "We'll be using experience replay memory for training our DQN. It stores\n",
    "the transitions that the agent observes, allowing us to reuse this data\n",
    "later. By sampling from it randomly, the transitions that build up a\n",
    "batch are decorrelated. It has been shown that this greatly stabilizes\n",
    "and improves the DQN training procedure.\n",
    "\n",
    "For this, we're going to need two classses:\n",
    "\n",
    "-  ``Transition`` - a named tuple representing a single transition in\n",
    "   our environment. It essentially maps (state, action) pairs\n",
    "   to their (next_state, reward) result, with the state being the\n",
    "   screen difference image as described later on.\n",
    "-  ``ReplayMemory`` - a cyclic buffer of bounded size that holds the\n",
    "   transitions observed recently. It also implements a ``.sample()``\n",
    "   method for selecting a random batch of transitions for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0#标记下一个transition存入的位置\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:#如果memory的大小还达不到capacity，则append一个None\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN算法\n",
    "-------------\n",
    "\n",
    "DQN的误差计算\n",
    "\n",
    "\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}\n",
    "\n",
    "优化时采用Huber loss：\n",
    "\n",
    "\\begin{align}\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\end{align}\n",
    "\n",
    "\\begin{align}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
    "     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
    "     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
    "   \\end{cases}\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============\n",
    "\n",
    "Q-network\n",
    "\n",
    "=============\n",
    "\n",
    "我们的模型将是一个卷积神经网络，该卷积神经网络将吸收当前屏幕补丁和先前屏幕补丁之间的差异。\n",
    "它有两个输出，分别代表𝑄（𝑠，left）和𝑄（𝑠，right）（其中𝑠是网络的输入）。\n",
    "实际上，网络正在尝试预测在给定当前输入的情况下采取每个操作的预期收益。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================\n",
    "\n",
    "Input extraction\n",
    "\n",
    "==================\n",
    "\n",
    "以下代码是用于从环境中提取和处理渲染图像的实用程序。get_screen()的shape为[1, 3, 40, 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # 车子的中心\n",
    "\n",
    "def get_screen():\n",
    "    # 返回 gym 需要的400x600x3 图片, 但有时会更大，如800x1200x3. 将其转换为torch (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # 车子在下半部分，因此请剥去屏幕的顶部和底部。\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # 去掉边缘，这样我们就可以得到一个以车为中心的正方形图像。\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # 转化为 float, 重新裁剪, 转化为 torch 张量(这并不需要拷贝)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # 重新裁剪,加入批维度 (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "--------\n",
    "\n",
    "==============================\n",
    "\n",
    "Hyperparameters and utilities\n",
    "\n",
    "==============================\n",
    "\n",
    "该单元实例化我们的模型及其优化器，并定义一些实用程序：\n",
    "\n",
    "-  ``select_action`` - 将根据epsilon贪婪策略选择一个操作。简而言之，我们有时会使用模型来选择动作，有时我们只会统一采样。选择随机动作的可能性将从EPS_START开始，并朝EPS_END呈指数衰减。 EPS_DECAY控制衰减率。\n",
    "\n",
    "-  ``plot_durations`` - 一个帮助绘制episodes持续时间以及最近100个episodes的平均值（官方评估中使用的度量）的助手。该episodes将在包含主要训练循环的单元格下方，并且将在每个episodes之后更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# 获取屏幕尺寸，以便我们可以根据形状正确初始化图层\n",
    "# shape为3x40x90\n",
    "\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# action的数量\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        #贪心\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        #随机\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============\n",
    "Training loop\n",
    "\n",
    "==============\n",
    "\n",
    "最后，是训练模型的代码。\n",
    "\n",
    "\n",
    "Here, you can find an ``optimize_model`` function that performs a\n",
    "single step of the optimization. It first samples a batch, concatenates\n",
    "all the tensors into a single one, computes $Q(s_t, a_t)$ and\n",
    "$V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$, and combines them into our\n",
    "loss. By defition we set $V(s) = 0$ if $s$ is a terminal\n",
    "state. We also use a target network to compute $V(s_{t+1})$ for\n",
    "added stability. The target network has its weights kept frozen most of\n",
    "the time, but is updated with the policy network's weights every so often.\n",
    "This is usually a set number of steps but we shall use episodes for\n",
    "simplicity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    #计算Q(s_t,a)，模型计算出Q(s_t)，然后我们选择出每个批次里a_t的Q值。\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # 计算 V(s_{t+1})\n",
    "    # non_final_next_states的预期动作值是根据“较旧的” target_net计算的；用max（1）[0]选择他们的最佳奖励。\n",
    "    # 这是根据掩码合并的，这样我们可以得到预期的状态值，而在状态为最终状态的情况下为0。\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # 计算预期的Q值\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下面，您可以找到主要的训练循环。首先，我们重置环境并初始化状态Tensor。然后，我们对一个动作进行采样，执行它，观察下一个屏幕和奖励（总是1），并一次优化我们的模型。当情节结束时（我们的模型失败），我们重新开始循环。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结：\n",
    "* expected_state_action_values即$(r + \\gamma \\max_a Q(s', a))$是用“旧的”target network计算，而state_action_values$Q(s,a)$用policy的target network计算。\n",
    "* \n",
    "\n",
    "问题：\n",
    "* memory的大小设置，memory如果不能及时更新会不会targt。\n",
    "* optimize_model()是在每一个episode执行一次还是在每一个episode的每一步执行一次。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bit04a86d77ae6f41f0bc78e941220ee1f3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
